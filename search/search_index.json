{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The grand chess tree is a public distributed effort to traverse the depths of the game of chess. The project started as a result of the enjoyment I found in the early days of working on Saplings move generator. If you're interested in the project please join the discord group Currently searching depth 12!","title":"Overview"},{"location":"dev/","text":"docker-compose up dotnet run docker buildx create --use ocker buildx build --platform linux/amd64,linux/arm64 -t aptacode/grand-chess-tree-worker:latest -t aptacode/grand-chess-tree-worker:0.0.2 -f .\\GrandChessTree.Client\\Dockerfile --push . docker buildx imagetools inspect aptacode/grand-chess-tree-worker:latest","title":"Dev"},{"location":"quickstart/","text":"Quick start Get in touch, you'll need an apikey in order to connect to the server. The easiest way is to join the discord group Download the latest client (windows, linux, mac are all supported (including ARM builds!)) Run the client and answer the questions with the following: api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> desired_depth: 12 That's it! **Note that you should pick a number of workers that corrosponds to less then the number of threads in your system since they will all be running in parrallel. ** You can close the program at any time, but do note that any progress on incomplete tasks will be lost. If you want to run from source (for apple silicon you may need to) git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" Alternatively you can also run the worker in docker docker run -e api_key=\"<your_api_key>\" -e api_url=\"https://api.grandchesstree.com/\" -e workers=4 -e depth=12 aptacode/grand-chess-tree-worker Making sense of the output worker: the index of the worker nps: the number of nodes per second the worker is able to process nodes: the number of nodes that have been processed for this task so far sub_tasks: completed / total sub tasks for the workers currently assigned tasks tasks: the number of full tasks that the worker has completed fen: the position the worker is currently processing The last three lines show: The number of subtasks that have been completed and the percentage of them that were duplicates of work you've already done The number of tasks you've submitted this session, and the number of tasks that you've completed but are waiting to be submitted Computed stats - this shows the amount and rate of nodes you're processing accross all of your workers (excluding caching) Effective stats - this shows the amount and rate of nodes that your sending to the server, it will be considerably higher then the rate your system can directly run the search since it includes cached nodes. | worker | nps | nodes | sub_tasks | tasks | fen | |--------|--------|--------|-----------|-------|-----------------------------------------------------------------| | 0 | 179.3m | 23.8b | 262/417 | 16 | rnbqkbnr/1ppppp1p/6p1/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 1 | 166.5m | 42.7b | 304/586 | 12 | rnbqkbnr/1ppp1ppp/4p3/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 2 | 144.6m | 92.6b | 469/565 | 11 | 1nbqkbnr/1ppppppp/r7/p7/8/P4N2/1PPPPPPP/RNBQKB1R w KQk - 0 1 | | 3 | 193.3m | 13.4b | 76/398 | 16 | rnbqkb1r/p1pppppp/1p5n/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 4 | 180.3m | 21.6b | 244/380 | 13 | rnbqkbnr/1pppppp1/7p/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 5 | 162.7m | 49.8b | 210/550 | 15 | rnbqkbnr/1pp1pppp/8/p2p4/8/P6N/1PPPPPPP/RNBQKB1R w KQkq d6 0 1 | | 6 | 164.1m | 375.6b | 612/752 | 12 | rnbqkbnr/1pp1pppp/8/p2p4/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq d6 0 1 | | 7 | 159.2m | 19.4b | 164/418 | 14 | rnbqkbnr/1pppppp1/8/p6p/8/P6N/1PPPPPPP/RNBQKB1R w KQkq h6 0 1 | | 8 | 180m | 56.9b | 380/530 | 18 | rnbqkbnr/1pp1pppp/3p4/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 9 | 180.9m | 354.6b | 596/803 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq e6 0 1 | | 10 | 169.4m | 174.7b | 549/618 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/6P1/P7/1PPPPP1P/RNBQKBNR w KQkq e6 0 1 | | 11 | 165.6m | 39.3b | 236/588 | 16 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P6N/1PPPPPPP/RNBQKB1R w KQkq e6 0 1 | | 12 | 159.1m | 104.3b | 358/579 | 14 | rnbqkbnr/1pp1pppp/8/p2p4/7P/P7/1PPPPPP1/RNBQKBNR w KQkq d6 0 1 | | 13 | 170.4m | 14.1b | 75/436 | 14 | rnbqkb1r/p1pppppp/1p3n2/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 14 | 150.5m | 18.3b | 245/417 | 17 | r1bqkbnr/1ppppppp/n7/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 15 | 158.5m | 30.9b | 314/379 | 16 | rnbqkbnr/1pppp1pp/5p2/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 16 | 162.7m | 13.6b | 224/436 | 11 | r1bqkbnr/1ppppppp/2n5/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 17 | 154m | 21.5b | 185/418 | 11 | rnbqkbnr/1ppppp1p/8/p5p1/8/P6N/1PPPPPPP/RNBQKB1R w KQkq g6 0 1 | | 18 | 199.1m | 105.7b | 525/558 | 13 | rnbqkbnr/1pp1pppp/3p4/p7/7P/P7/1PPPPPP1/RNBQKBNR w KQkq - 0 1 | | 19 | 174.7m | 4.8b | 42/422 | 11 | rnbqkbnr/p1pppppp/8/1p6/8/P1P5/1P1PPPPP/RNBQKBNR w KQkq - 0 1 | | 20 | 196.6m | 28.9b | 190/397 | 16 | rnbqkbnr/1pppp1pp/8/p4p2/8/P6N/1PPPPPPP/RNBQKB1R w KQkq f6 0 1 | | 21 | 170.9m | 14.1b | 72/436 | 15 | r1bqkbnr/p1pppppp/1pn5/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 22 | 160.1m | 4.6b | 42/510 | 12 | rn1qkbnr/pbpppppp/1p6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 23 | 176.7m | 5b | 30/432 | 3 | rn1qkbnr/p1pppppp/bp6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | completed 159k subtasks (29% cache hits), submitted 322 tasks (0 pending) [computed stats] 39.3t nodes at 4.1bnps [effective stats] 129.6t nodes at 13.3bnps","title":"Quick start"},{"location":"quickstart/#quick-start","text":"Get in touch, you'll need an apikey in order to connect to the server. The easiest way is to join the discord group Download the latest client (windows, linux, mac are all supported (including ARM builds!)) Run the client and answer the questions with the following: api_url: https://api.grandchesstree.com/ api_key: <your_api_key> workers: <number_of_threads> desired_depth: 12 That's it! **Note that you should pick a number of workers that corrosponds to less then the number of threads in your system since they will all be running in parrallel. ** You can close the program at any time, but do note that any progress on incomplete tasks will be lost. If you want to run from source (for apple silicon you may need to) git clone https://github.com/Timmoth/grandchesstree cd GrandChessTree.Client # x86 dotnet run -c Release --no-launch-profile # ARM dotnet run -c Release --no-launch-profile --property:DefineConstants=\"ARM\" Alternatively you can also run the worker in docker docker run -e api_key=\"<your_api_key>\" -e api_url=\"https://api.grandchesstree.com/\" -e workers=4 -e depth=12 aptacode/grand-chess-tree-worker","title":"Quick start"},{"location":"quickstart/#making-sense-of-the-output","text":"worker: the index of the worker nps: the number of nodes per second the worker is able to process nodes: the number of nodes that have been processed for this task so far sub_tasks: completed / total sub tasks for the workers currently assigned tasks tasks: the number of full tasks that the worker has completed fen: the position the worker is currently processing The last three lines show: The number of subtasks that have been completed and the percentage of them that were duplicates of work you've already done The number of tasks you've submitted this session, and the number of tasks that you've completed but are waiting to be submitted Computed stats - this shows the amount and rate of nodes you're processing accross all of your workers (excluding caching) Effective stats - this shows the amount and rate of nodes that your sending to the server, it will be considerably higher then the rate your system can directly run the search since it includes cached nodes. | worker | nps | nodes | sub_tasks | tasks | fen | |--------|--------|--------|-----------|-------|-----------------------------------------------------------------| | 0 | 179.3m | 23.8b | 262/417 | 16 | rnbqkbnr/1ppppp1p/6p1/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 1 | 166.5m | 42.7b | 304/586 | 12 | rnbqkbnr/1ppp1ppp/4p3/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 2 | 144.6m | 92.6b | 469/565 | 11 | 1nbqkbnr/1ppppppp/r7/p7/8/P4N2/1PPPPPPP/RNBQKB1R w KQk - 0 1 | | 3 | 193.3m | 13.4b | 76/398 | 16 | rnbqkb1r/p1pppppp/1p5n/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 4 | 180.3m | 21.6b | 244/380 | 13 | rnbqkbnr/1pppppp1/7p/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 5 | 162.7m | 49.8b | 210/550 | 15 | rnbqkbnr/1pp1pppp/8/p2p4/8/P6N/1PPPPPPP/RNBQKB1R w KQkq d6 0 1 | | 6 | 164.1m | 375.6b | 612/752 | 12 | rnbqkbnr/1pp1pppp/8/p2p4/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq d6 0 1 | | 7 | 159.2m | 19.4b | 164/418 | 14 | rnbqkbnr/1pppppp1/8/p6p/8/P6N/1PPPPPPP/RNBQKB1R w KQkq h6 0 1 | | 8 | 180m | 56.9b | 380/530 | 18 | rnbqkbnr/1pp1pppp/3p4/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 9 | 180.9m | 354.6b | 596/803 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P2P4/1PP1PPPP/RNBQKBNR w KQkq e6 0 1 | | 10 | 169.4m | 174.7b | 549/618 | 13 | rnbqkbnr/1ppp1ppp/8/p3p3/6P1/P7/1PPPPP1P/RNBQKBNR w KQkq e6 0 1 | | 11 | 165.6m | 39.3b | 236/588 | 16 | rnbqkbnr/1ppp1ppp/8/p3p3/8/P6N/1PPPPPPP/RNBQKB1R w KQkq e6 0 1 | | 12 | 159.1m | 104.3b | 358/579 | 14 | rnbqkbnr/1pp1pppp/8/p2p4/7P/P7/1PPPPPP1/RNBQKBNR w KQkq d6 0 1 | | 13 | 170.4m | 14.1b | 75/436 | 14 | rnbqkb1r/p1pppppp/1p3n2/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 14 | 150.5m | 18.3b | 245/417 | 17 | r1bqkbnr/1ppppppp/n7/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 15 | 158.5m | 30.9b | 314/379 | 16 | rnbqkbnr/1pppp1pp/5p2/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 16 | 162.7m | 13.6b | 224/436 | 11 | r1bqkbnr/1ppppppp/2n5/p7/8/P6N/1PPPPPPP/RNBQKB1R w KQkq - 0 1 | | 17 | 154m | 21.5b | 185/418 | 11 | rnbqkbnr/1ppppp1p/8/p5p1/8/P6N/1PPPPPPP/RNBQKB1R w KQkq g6 0 1 | | 18 | 199.1m | 105.7b | 525/558 | 13 | rnbqkbnr/1pp1pppp/3p4/p7/7P/P7/1PPPPPP1/RNBQKBNR w KQkq - 0 1 | | 19 | 174.7m | 4.8b | 42/422 | 11 | rnbqkbnr/p1pppppp/8/1p6/8/P1P5/1P1PPPPP/RNBQKBNR w KQkq - 0 1 | | 20 | 196.6m | 28.9b | 190/397 | 16 | rnbqkbnr/1pppp1pp/8/p4p2/8/P6N/1PPPPPPP/RNBQKB1R w KQkq f6 0 1 | | 21 | 170.9m | 14.1b | 72/436 | 15 | r1bqkbnr/p1pppppp/1pn5/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 22 | 160.1m | 4.6b | 42/510 | 12 | rn1qkbnr/pbpppppp/1p6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | | 23 | 176.7m | 5b | 30/432 | 3 | rn1qkbnr/p1pppppp/bp6/8/1P6/P7/2PPPPPP/RNBQKBNR w KQkq - 0 1 | completed 159k subtasks (29% cache hits), submitted 322 tasks (0 pending) [computed stats] 39.3t nodes at 4.1bnps [effective stats] 129.6t nodes at 13.3bnps","title":"Making sense of the output"},{"location":"results/","text":"Here is the table of results for the initial position up to depth 11, depth 12 coming soon! | depth | nodes | captures | enpassants | castles | promotions | direct_checks | single_discovered_checks | direct_discovered_checks | double_discovered_check | total_checks | direct_mates | single_discovered_mates | direct_discoverd_mates | double_discoverd_mates | total_mates | |-------|------------------|-----------------|--------------|---------------|-------------|----------------|--------------------------|--------------------------|-------------------------|----------------|--------------|-------------------------|------------------------|------------------------|--------------| | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 1 | 20 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 2 | 400 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | 3 | 8902 | 34 | 0 | 0 | 0 | 12 | 0 | 0 | 0 | 12 | 0 | 0 | 0 | 0 | 0 | | 4 | 197281 | 1576 | 0 | 0 | 0 | 461 | 0 | 0 | 0 | 461 | 8 | 0 | 0 | 0 | 8 | | 5 | 4865609 | 82719 | 258 | 0 | 0 | 26998 | 6 | 0 | 0 | 27004 | 347 | 0 | 0 | 0 | 347 | | 6 | 119060324 | 2812008 | 5248 | 0 | 0 | 797896 | 329 | 46 | 0 | 798271 | 10828 | 0 | 0 | 0 | 10828 | | 7 | 3195901860 | 108329926 | 319617 | 883453 | 0 | 32648427 | 18026 | 1628 | 0 | 32668081 | 435767 | 0 | 0 | 0 | 435767 | | 8 | 84998978956 | 3523740106 | 7187977 | 23605205 | 0 | 958135303 | 847039 | 147215 | 0 | 959129557 | 9852032 | 4 | 0 | 0 | 9852036 | | 9 | 2439530234167 | 125208536153 | 319496827 | 1784356000 | 17334376 | 35653060996 | 37101713 | 5547221 | 10 | 35695709940 | 399421379 | 1869 | 768715 | 0 | 400191963 | | 10 | 69352859712417 | 4092784875884 | 7824835694 | 50908510199 | 511374376 | 1077020493859 | 1531274015 | 302900733 | 879 | 1078854669486 | 8771693969 | 598058 | 18327128 | 0 | 8790619155 | | 11 | 2097651003696806 | 142537161824567 | 313603617408 | 2641343463566 | 49560932860 | 39068470901662 | 67494850305 | 11721852393 | 57443 | 39147687661803 | 360675926605 | 60344676 | 1553739626 | 0 | 362290010907 |","title":"Results"},{"location":"technical/","text":"There are four main components to the system: Database We're using PostgreSQL to store tasks, results, accounts, and API keys, with EF Core as the ORM. Tables: accounts \u2013 Stores basic information on each user, including ID, name, email, and role. api_keys \u2013 Stores authentication information (the API key itself is stored in a hashed format). perft_items \u2013 At each depth, we split the search into 101,240 tasks. This is done by first searching to depth 4 (197,281 positions), then storing a task for each of the 101,240 unique positions reachable, along with the number of times it occurs. The results from each worker task are multiplied by the occurrences. perft_tasks \u2013 Each row corresponds to the results for a specific task at a certain depth, computed by a single user. To verify correctness, multiple entries will exist for each task/depth, contributed by different users using different hashes. API We use .NET for the API, which is containerized using Docker and deployed on a VPS. The API is fairly straightforward and allows the client to: Query for a batch of tasks (using row-level locking to ensure concurrent requests don't receive the same tasks). Submit a batch of results to be stored in the database. Query for real-time and total analytics. Query for compiled results. Note: The analytics/results endpoints use response caching, so they may take a few minutes to update. Web App We have a React/TypeScript frontend using Tailwind CSS, which provides an interface for viewing the current progress of the active search, including the leaderboard. Client The client is a .NET console app, published using GitHub Actions as a self-contained binary for each platform (Windows, Linux, macOS). The client performs the actual computations. After gathering initial configuration details (stored in ./gct_config.json ), it periodically requests batches of work from the server to ensure tasks are always available. Each task requested from the server corresponds to a unique position occurring at depth 4. The client then performs a quick search to depth 2 (cumulatively reaching depth 6) to split the task into subtasks. These subtasks are deduplicated to avoid redundant searches. The client-side subtask result table is also checked to see if a position has already been processed within the session. The remaining positions are then searched to the final depth ( desired_depth - 6 ). Each worker is assigned its own task and searches serially to the final depth, periodically updating the console with progress. Once a worker completes a task, it queues the result for submission to the API before requesting the next task. Note: Choose a number of workers that is less than the number of threads available on your system since they will all run in parallel. Search Features The search algorithm employs several common techniques used in chess engines: Zobrist hashing for efficient position lookups. PEXT lookup for sliding piece moves. Legal move generation using various masks for pinned pieces, attackers, etc. Copy-make technique for certain non-legal move generations (e.g., en passant moves). Condensed board state using bitboards for pawns, knights, bishops, rooks, queens, white occupancy, and black occupancy. Pointer-based memory allocation and aligned memory to minimize bounds checking in .NET. Stack-allocated structs to reduce garbage collection overhead. There is some inherent inconsistency when using a hash table due to potential collisions. To mitigate this, a unique method is used where the full hash is XORed with the occupancy when checking for collisions. If 12 bits from the original hash are used to look up the relevant entry, these bits are then XORed again with the occupancy so that they perform a different comparison when verifying a match.","title":"Technical overview"},{"location":"technical/#database","text":"We're using PostgreSQL to store tasks, results, accounts, and API keys, with EF Core as the ORM.","title":"Database"},{"location":"technical/#tables","text":"accounts \u2013 Stores basic information on each user, including ID, name, email, and role. api_keys \u2013 Stores authentication information (the API key itself is stored in a hashed format). perft_items \u2013 At each depth, we split the search into 101,240 tasks. This is done by first searching to depth 4 (197,281 positions), then storing a task for each of the 101,240 unique positions reachable, along with the number of times it occurs. The results from each worker task are multiplied by the occurrences. perft_tasks \u2013 Each row corresponds to the results for a specific task at a certain depth, computed by a single user. To verify correctness, multiple entries will exist for each task/depth, contributed by different users using different hashes.","title":"Tables:"},{"location":"technical/#api","text":"We use .NET for the API, which is containerized using Docker and deployed on a VPS. The API is fairly straightforward and allows the client to: Query for a batch of tasks (using row-level locking to ensure concurrent requests don't receive the same tasks). Submit a batch of results to be stored in the database. Query for real-time and total analytics. Query for compiled results. Note: The analytics/results endpoints use response caching, so they may take a few minutes to update.","title":"API"},{"location":"technical/#web-app","text":"We have a React/TypeScript frontend using Tailwind CSS, which provides an interface for viewing the current progress of the active search, including the leaderboard.","title":"Web App"},{"location":"technical/#client","text":"The client is a .NET console app, published using GitHub Actions as a self-contained binary for each platform (Windows, Linux, macOS). The client performs the actual computations. After gathering initial configuration details (stored in ./gct_config.json ), it periodically requests batches of work from the server to ensure tasks are always available. Each task requested from the server corresponds to a unique position occurring at depth 4. The client then performs a quick search to depth 2 (cumulatively reaching depth 6) to split the task into subtasks. These subtasks are deduplicated to avoid redundant searches. The client-side subtask result table is also checked to see if a position has already been processed within the session. The remaining positions are then searched to the final depth ( desired_depth - 6 ). Each worker is assigned its own task and searches serially to the final depth, periodically updating the console with progress. Once a worker completes a task, it queues the result for submission to the API before requesting the next task. Note: Choose a number of workers that is less than the number of threads available on your system since they will all run in parallel.","title":"Client"},{"location":"technical/#search-features","text":"The search algorithm employs several common techniques used in chess engines: Zobrist hashing for efficient position lookups. PEXT lookup for sliding piece moves. Legal move generation using various masks for pinned pieces, attackers, etc. Copy-make technique for certain non-legal move generations (e.g., en passant moves). Condensed board state using bitboards for pawns, knights, bishops, rooks, queens, white occupancy, and black occupancy. Pointer-based memory allocation and aligned memory to minimize bounds checking in .NET. Stack-allocated structs to reduce garbage collection overhead. There is some inherent inconsistency when using a hash table due to potential collisions. To mitigate this, a unique method is used where the full hash is XORed with the occupancy when checking for collisions. If 12 bits from the original hash are used to look up the relevant entry, these bits are then XORed again with the occupancy so that they perform a different comparison when verifying a match.","title":"Search Features"}]}